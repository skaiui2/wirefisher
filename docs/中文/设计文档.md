## wirefisher

本项目的核心在于两个地方：流量监控与限速。

## 流量监控

流速计算：分为瞬时流速计算和平滑流速计算

瞬时流速计算：到达数据包大小/数据包到达时间间隔

瞬时流速无法反映平均情况，因此又额外加了平滑流速计算

平滑流速计算：7/8历史流速 + 1/8新流速

这里使用的是Mbps，8Mbs = 1MB.

代码：

```
位于/bpf/common.h

static __inline void update_flow_rate(struct flow_rate_info *flow_info, __u32 packet_size) 
{
    __u64 now = start_to_now_ns();
    flow_info->total_bytes += packet_size;
    flow_info->rate_bps = (flow_info->total_bytes * NSEC_PER_SEC) / (now - flow_info->window_start_ns);
    if (now - flow_info->last_ns >= NSEC_PER_SEC) {
        flow_info->instance_rate_bps = (flow_info->packet_bytes * NSEC_PER_SEC) / (now - flow_info->last_ns);
        if (flow_info->instance_rate_bps > flow_info->peak_rate_bps) {
            flow_info->peak_rate_bps = flow_info->instance_rate_bps;
        }

        if (flow_info->smooth_rate_bps != 0) {
            flow_info->smooth_rate_bps = (flow_info->smooth_rate_bps - (flow_info->smooth_rate_bps >> 3)) + (flow_info->instance_rate_bps  >> 3);
        } else {
            flow_info->smooth_rate_bps = flow_info->instance_rate_bps;
        }
        flow_info->last_ns = now;
        flow_info->packet_bytes = packet_size;
    } else {
        flow_info->packet_bytes += packet_size;
    }
}

```

## 限速

网络限流的核心在于令牌桶算法。

即通过丢包实现网络流量控制，这一做法在java web后端很常见，主要使用的算法为计数器、令牌桶、漏桶这几种算法。

考虑到网络层无法实现阻塞，因此使用令牌桶算法。

## 令牌桶算法

使用一个数据结构记录令牌，以设定好的速度增加令牌，其中令牌的大小就是字节数。

当一个数据包到达时，我们按照设定的速度给令牌桶中增加令牌，然后判断当前数据包的大小是否超过桶中的令牌数，如果没有超过，那么减去桶中的令牌数，然后放行该数据包。如果发现数据包大小超过桶中令牌数，那么丢弃该数据包。

### 代码实现

```
位于/bpf/common.h

static __always_inline int rate_limit_check(struct rate_limit *rate)
{
    __u64 now = bpf_ktime_get_ns();
    __u64 delta_ns;
    struct rate_bucket *b;
    
    __u64 max_bucket = (rate->rate_bps * rate->time_scale) >> 2;

    b = bpf_map_lookup_elem(rate->buckets, rate->bucket_key);
    if (!b) {
        struct rate_bucket init = { 
            .ts_ns = now, 
            .tokens = max_bucket
        };
        bpf_map_update_elem(rate->buckets, rate->bucket_key, &init, 0);
        b = bpf_map_lookup_elem(rate->buckets, rate->bucket_key);
        if (!b) {
            return ACCEPT;
        }
    }

    delta_ns = now - b->ts_ns;
    b->tokens += (delta_ns * rate->rate_bps) / NSEC_PER_SEC;
    if (b->tokens > max_bucket) {
        b->tokens = max_bucket;
    }

    b->ts_ns = now;

    if (b->tokens < rate->packet_len) {
        return DROP;
    }

    b->tokens -= rate->packet_len;

    return ACCEPT;
}
```



### 问题

但是现在会有一个问题，桶中的令牌是持续累计的，如果一个程序运行了非常久都没有流量，然后桶中累计了大量令牌，那么是不是会导致后面无法限流呢？

限制突发流量的核心在于：桶的大小设定了上限。

在程序中，桶的大小 = 1/8(设定速率 * 设定时刻)

这样可以保证，即使有突发流量，在突发时刻流速最大也不会超过设定的1/8。

例如设定了1M/s，突发流量最多消耗1/8桶容量令牌 + 1M。

因此，限流的最大瞬时误差为12.5%。

当然，令牌桶太小了也不行，不然任何数据包都无法通过。



## ip、协议、端口限流

实现功能：ip、协议、端口限流，这三个可以叠加使用，也可以单独使用

hook点：netfilter

这一层，sk->data指向ip网际层

基本实现思路：

使用ip_hdr, tcp_hdr, udp_hdr这些函数加载数据包，解析出包中的ip，协议及端口，然后使用令牌桶算法进行限流即可。



## cgroup限流

### 注意

如果要新建组，然后将测试进程拉入组内，然后观察是否限流，这种做法有一点需要注意：

socket的建立是不会随进程迁移的，所以应该在测试时，先使用getchar等函数阻塞socket的建立，然后将测试进程拉入cgroup组内，这样就可以观察了。

当然，实际进程也不会任意迁移，在容器场景，直接在开始时限制一整个cgroup即可，后续的进程都会被限流。

以及：

在shell终端中，创建的进程都是当前shell的子进程，都位于cgroup树的下面，所以，如果你的测试进程创建套接字是在没有迁移进程之前创建的，直接对shell终端所在的路径进行限流，我们会发现子进程也被限流了



## 进程限流

hook点：netfilter层

在这里，sk->data指向ip头部

实现思路：使用sock与进程信息进行映射，当数据包在netfilter这里被捕获时，通过sk_buff中的sock指针获取进程信息。

我们使用的是NF_INET_LOCAL_IN:

```
  // Attach to ingress hook (NF_INET_LOCAL_IN)
    attr.link_create.prog_fd = bpf_program__fd(skel->progs.netfilter_hook);
    attr.link_create.attach_type = BPF_NETFILTER;
    attr.link_create.netfilter.pf = NFPROTO_IPV4;
    attr.link_create.netfilter.hooknum = NF_INET_LOCAL_IN;
    attr.link_create.netfilter.priority = -128;
```

但是在实际使用中，我发现虽然tcp能进行进程限流，但是udp接收时却不能，这是因为udp是无连接的协议，在传输层的处理实际上非常简单，就是单播、多播、广播的判断与运行，可能实际上并没有设置sock指针。

在stackoverflow，我发现有人遇到了同样的问题：

https://stackoverflow.com/questions/44146312/receiving-socket-information-from-netfilter-nf-inet-pre-routing-hook-function-in

### UDP接收的单独处理

为了解决这个问题，我使用网络元组进行映射，

ip+协议+端口，这其实已经能够映射到一个进程。

在socket中，当udp接收时，存储三元组与进程信息的映射，然后在数据包接收触发hook程序时，如果解析出来是udp数据包，那么就使用网络元组进行查询。



## 测试程序

测试基本思路：一个发送进程、一个接收进程，启动限流程序，观察接收端接收到的流量是否符合限流结果。

不管是发送还是接收，判断依据都是接收进程接收到的流量，因为我们很难直接捕获发送端到达发送了多少流量，但是我们知道这些流量必然发往接收端。

测试平台：Windows主机与ubuntu25虚拟机

### windows主机服务端

```
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <stdbool.h>
#include <winsock2.h>
#include <windows.h>
#include <time.h>

#pragma comment(lib, "ws2_32.lib")

#define TCP_PORT           9090
#define MAX_DATA_SIZE      1472

static volatile bool tcp_running = true;

// 信号处理模拟（Windows 没有 SIGINT 捕获机制，用控制台事件）
BOOL WINAPI ConsoleHandler(DWORD signal) {
    if (signal == CTRL_C_EVENT || signal == CTRL_CLOSE_EVENT) {
        printf("[SERVER] Received shutdown signal, exiting...\n");
        tcp_running = false;
    }
    return TRUE;
}

// 日志输出
void log_info(const char *msg) {
    printf("[SERVER] %s\n", msg);
}

void log_error(const char *msg, int err) {
    fprintf(stderr, "[SERVER][ERROR] %s: %d\n", msg, err);
}

// 生成随机数据包大小
int generate_tcp_packet_size() {
    double u = (double)rand() / RAND_MAX;
    if (u < 0.4) {
        return 64 + (rand() % (512 - 64 + 1));
    } else if (u < 0.7) {
        return 512 + (rand() % (1024 - 512 + 1));
    } else if (u < 0.9) {
        return 1024 + (rand() % (1400 - 1024 + 1));
    } else {
        return 1400 + (rand() % (1472 - 1400 + 1));
    }
}

// 生成随机发送间隔（微秒）
int generate_send_interval() {
    double u = (double)rand() / RAND_MAX;
    if (u < 0.3) {
        return 1 + (rand() % 9);
    } else if (u < 0.6) {
        return 10 + (rand() % 90);
    } else if (u < 0.8) {
        return 100 + (rand() % 900);
    } else {
        return 1000 + (rand() % 9000);
    }
}

int main() {
    WSADATA wsa;
    SOCKET server_fd, client_fd;
    struct sockaddr_in server_addr, client_addr;
    int client_len = sizeof(client_addr);
    char data[MAX_DATA_SIZE];
    int bytes_sent;
    time_t start_time, current_time, last_report_time;
    long total_sent = 0;
    int packets_sent = 0;
    int burst_count = 0;
    bool burst_mode = false;
    double current_rate_multiplier = 1.0;
    time_t last_rate_change;

    srand((unsigned int)time(NULL));
    SetConsoleCtrlHandler(ConsoleHandler, TRUE);

    if (WSAStartup(MAKEWORD(2, 2), &wsa) != 0) {
        log_error("WSAStartup failed", WSAGetLastError());
        return 1;
    }

    server_fd = socket(AF_INET, SOCK_STREAM, 0);
    if (server_fd == INVALID_SOCKET) {
        log_error("socket() failed", WSAGetLastError());
        return 1;
    }

    int opt = 1;
    setsockopt(server_fd, SOL_SOCKET, SO_REUSEADDR, (char*)&opt, sizeof(opt));

    server_addr.sin_family = AF_INET;
    server_addr.sin_addr.s_addr = INADDR_ANY;
    server_addr.sin_port = htons(TCP_PORT);

    if (bind(server_fd, (struct sockaddr*)&server_addr, sizeof(server_addr)) == SOCKET_ERROR) {
        log_error("bind() failed", WSAGetLastError());
        closesocket(server_fd);
        WSACleanup();
        return 1;
    }

    if (listen(server_fd, 5) == SOCKET_ERROR) {
        log_error("listen() failed", WSAGetLastError());
        closesocket(server_fd);
        WSACleanup();
        return 1;
    }

    log_info("Listening on port 9090...");
    log_info("Waiting for client connection...");

    client_fd = accept(server_fd, (struct sockaddr*)&client_addr, &client_len);
    if (client_fd == INVALID_SOCKET) {
        log_error("accept() failed", WSAGetLastError());
        closesocket(server_fd);
        WSACleanup();
        return 1;
    }

    printf("[SERVER] Client connected: %s:%d\n",
           inet_ntoa(client_addr.sin_addr),
           ntohs(client_addr.sin_port));

    for (int i = 0; i < MAX_DATA_SIZE; i++) {
        data[i] = 'A' + (rand() % 26);
    }

    start_time = time(NULL);
    last_report_time = start_time;
    last_rate_change = start_time;

    log_info("Start sending data...");
    log_info("Time    Packets Sent    Bytes Sent    Bandwidth (MiB/s)");

    while (tcp_running) {
        current_time = time(NULL);

        if (current_time - last_rate_change >= 1) {
            double u = (double)rand() / RAND_MAX;
            if (u < 0.2)         current_rate_multiplier = 10.0;
            else if (u < 0.4)    current_rate_multiplier = 5.0;
            else if (u < 0.6)    current_rate_multiplier = 2.0;
            else if (u < 0.8)    current_rate_multiplier = 1.0;
            else                 current_rate_multiplier = 0.5;
            last_rate_change = current_time;
        }

        int delay_us;
        if (burst_mode && burst_count < 50) {
            delay_us = 0;
            burst_count++;
        } else {
            burst_mode = false;
            delay_us = (int)(generate_send_interval() / current_rate_multiplier);
            if (((double)rand() / RAND_MAX) < 0.3) {
                burst_mode = true;
                burst_count = 0;
            }
        }

        int packet_size = generate_tcp_packet_size();
        bytes_sent = send(client_fd, data, packet_size, 0);
        if (bytes_sent <= 0) {
            log_info("Client disconnected or send error");
            break;
        }

        total_sent += bytes_sent;
        packets_sent++;

        if (current_time != last_report_time) {
            double mib_sent = total_sent / (1024.0 * 1024.0);
            time_t elapsed = current_time - start_time;
            printf("%02ld:%02ld    %8d        %10ld    %8.2f\n",
                   elapsed / 60, elapsed % 60,
                   packets_sent, total_sent, mib_sent);

            total_sent = 0;
            packets_sent = 0;
            last_report_time = current_time;
        }

        if (delay_us > 0)
            Sleep(delay_us / 1000); // 微秒转毫秒
    }

    log_info("Shutting down server");
    closesocket(client_fd);
    closesocket(server_fd);
    WSACleanup();
    return 0;
}

```

### ubuntu虚拟机客户端

测试时记得更改服务器ip：

```
#include <iostream>
#include <string>
#include <cstring>
#include <cstdlib>
#include <ctime>
#include <csignal>
#include <unistd.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <arpa/inet.h>

#define SERVER_IP               "192.168.46.5" //这里需要更改为你的windows主机的ip
#define TCP_CLIENT_PORT         9090
#define TCP_CLIENT_BUFFER_SIZE  2048

static volatile bool tcp_client_running = true;

static void tcp_client_signal_handler(int sig) {
    std::cout << "[CLIENT] Received signal " << sig << ", exiting...\n";
    tcp_client_running = false;
}

static void log_info(const std::string &msg) {
    std::cout << "[CLIENT] " << msg << "\n";
}

static void log_error(const std::string &msg, int err) {
    std::cerr << "[CLIENT][ERROR] " << msg << ": " << strerror(err) << "\n";
}
int main() {
    int client_fd = -1;
    struct sockaddr_in server_addr{};
    char buffer[TCP_CLIENT_BUFFER_SIZE];
    ssize_t bytes_received;
    long total_received = 0;
    int packets_received = 0;
    time_t start_time, current_time, last_report_time;

    pid_t client_pid = getpid();
    log_info("Client PID: " + std::to_string(client_pid));
    // 安装信号处理
    std::signal(SIGINT,  tcp_client_signal_handler);
    std::signal(SIGTERM, tcp_client_signal_handler);

    // 创建 socket
    client_fd = socket(AF_INET, SOCK_STREAM, 0);
    if (client_fd < 0) {
        log_error("socket() failed", errno);
        return EXIT_FAILURE;
    }

    // 配置服务器地址
    server_addr.sin_family = AF_INET;
    server_addr.sin_port   = htons(TCP_CLIENT_PORT);
    if (inet_pton(AF_INET, SERVER_IP, &server_addr.sin_addr) <= 0) {
        log_error("inet_pton() failed", errno);
        close(client_fd);
        return EXIT_FAILURE;
    }

    log_info("Connecting to " SERVER_IP ":" + std::to_string(TCP_CLIENT_PORT));

    // 发起连接
    if (connect(client_fd,
                (struct sockaddr*)&server_addr,
                sizeof(server_addr)) < 0) {
        log_error("connect() failed", errno);
        close(client_fd);
        return EXIT_FAILURE;
    }

    log_info("Connected. Receiving data...");
    log_info("Time    Packets Recv    Bytes Recv    Bandwidth (MiB/s)");

    start_time       = time(nullptr);
    last_report_time = start_time;

    // 主接收循环
    while (tcp_client_running) {
        bytes_received = recv(client_fd, buffer, sizeof(buffer), 0);
        if (bytes_received < 0) {
            log_error("recv() failed", errno);
            break;
        }
        if (bytes_received == 0) {
            log_info("Server closed connection");
            break;
        }

        total_received   += bytes_received;
        packets_received++;
        current_time      = time(nullptr);

        // 每秒输出一次统计
        if (current_time != last_report_time) {
            double mib_received = total_received / (1024.0 * 1024.0);
            time_t elapsed = current_time - start_time;
            char line[256];
            std::snprintf(line, sizeof(line),
                "%02ld:%02ld    %6d        %10ld    %6.2f",
                elapsed/60, elapsed%60,
                packets_received, total_received, mib_received);
            log_info(line);

            // 重置计数
            total_received   = 0;
            packets_received = 0;
            last_report_time = current_time;
        }
    }

    // 清理并退出
    log_info("Client shutting down");
    if (client_fd >= 0) {
        close(client_fd);
    }
    return 0;
}


```

我们分别在两个平台编译并进行测试。

现在我们可以修改/config/config.yaml中的文件内容进行测试：

例如，在测试程序启动时，会打印pid:

```
skaiuijing@skaiuijing-VMware-Virtual-Platform:~/Documents/test$ sudo ./tcp_client 
[CLIENT] Client PID: 4261
[CLIENT] Connecting to 10.149.91.193:9090
[CLIENT] Connected. Receiving data...
[CLIENT] Time    Packets Recv    Bytes Recv    Bandwidth (MiB/s)
[CLIENT] 00:01       435            846012      0.81
[CLIENT] 00:02      1695           3293976      3.14
[CLIENT] 00:03      1166           2253468      2.15
```

我们可以指定pid及限速：

```
process_module:
  process_rule:
    target_pid: 4261
    rate_bps: 1M
    gress: ingress
    time_scale: 1s

```

未限速情况下发送端流速是，我们可以将rate_bps设置为1000M这样观察正常流速：

```
=== process_traffic ===
 instant_rate_bps : 2.00 MB/s
 rate_bps         : 23.11 MB/s
 peak_rate_bps    : 92.49 MB/s
 smoothed_rate_bps: 22.60 MB/s
 timestamp         : 00:12:45.675
```

现在让我们启用wirefisher：

观察tcp客户端的打印，限速是否成功:

```
[CLIENT] 01:41     11233          22476009     21.43
[CLIENT] 01:42      1989           3932812      3.75
[CLIENT] 01:43     60978         116807328    111.40
[CLIENT] 01:44     12987          26227825     25.01
[CLIENT] 01:45     60930         117834217    112.38
[CLIENT] 01:54     43581          87821733     83.75
[CLIENT] 01:55     14646          29144493     27.79
[CLIENT] 01:56      2545           4924922      4.70
[CLIENT] 01:57      2439           4730565      4.51
[CLIENT] 01:59     51165         101517208     96.81
[CLIENT] 02:00     37580          75549487     72.05//上面是限速启用前，下面是限速启用后：
[CLIENT] 02:01       511           1027824      0.98
[CLIENT] 02:02       576           1156302      1.10
[CLIENT] 02:03       512           1027824      0.98
[CLIENT] 02:04       498           1000085      0.95
[CLIENT] 02:05       500           1003005      0.96
[CLIENT] 02:06       527           1057025      1.01
[CLIENT] 02:07       480            963585      0.92
[CLIENT] 02:08       544           1092063      1.04
[CLIENT] 02:09       544           1092063      1.04
[CLIENT] 02:10       512           1027824      0.98
[CLIENT] 02:11       512           1027824      0.98
[CLIENT] 02:12       512           1027824      0.98
[CLIENT] 02:13       512           1027824      0.98
[CLIENT] 02:14       507           1020525      0.97
[CLIENT] 02:15       529           1065784      1.02
[CLIENT] 02:16       478            963585      0.92
[CLIENT] 02:17       561           1128563      1.08
[CLIENT] 02:18       523           1051185      1.00
[CLIENT] 02:19       512           1027824      0.98
```

可以看出限速还是相当平稳的。

wirefisher打印输出的流速：

```
=== process_traffic ===
 instant_rate_bps : 1.53 MB/s
 rate_bps         : 1.56 MB/s
 peak_rate_bps    : 1.66 MB/s
 smoothed_rate_bps: 1.55 MB/s
 timestamp         : 00:07:55.914
```

这是因为tcp是面向连接的协议，在我们启用限速后，由于接收方大量数据包未接收，因此tcp发送方判定为网络拥塞，发送速度也减慢了。
